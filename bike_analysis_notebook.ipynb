{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37172faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "from tools import DataTools\n",
    "# Importing necessary modules\n",
    "from data import Data\n",
    "# Importing necessary modules\n",
    "from datetime import datetime\n",
    "# Importing necessary modules\n",
    "import pandas as pd\n",
    "# Importing necessary modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Importing necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "# Importing necessary modules\n",
    "import seaborn as sns\n",
    "# Importing necessary modules\n",
    "import gc\n",
    "# Add import for the time module\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b465e8",
   "metadata": {},
   "source": [
    "### üïí √âtape 1 : D√©finir la date d'analyse\n",
    "Dans cette √©tape, nous d√©finissons une date et une heure sp√©cifiques √† analyser. Cette date servira √† charger les fichiers de donn√©es correspondants √† l‚Äôann√©e choisie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4244af",
   "metadata": {},
   "source": [
    "Notre code fonctionne de mani√®re flexible, si l'on veut toutes les donn√©es d'une ann√©e il suffit de renseigner l'ann√©e souhait√©e, si l'on veut le d√©tail sur un mois, on lui donne le mois, et si l'on veut le d√©tail que sur certains jours on peut aussi lui sp√©cifier la date et l'heure d'un jour en particulier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b7f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "specified_time = \"2022-11-29 10:00\"\n",
    "# Convert string to datetime object\n",
    "specified_time = datetime.strptime(specified_time, \"%Y-%m-%d %H:%M\")\n",
    "# Format the datetime object to extract the year\n",
    "formatted_time = specified_time.strftime(\"%Y\")\n",
    "\n",
    "# URL de l'API JCDecaux pour r√©cup√©rer tous les contrats\n",
    "contracts_url = \"https://api.jcdecaux.com/vls/v3/contracts\"\n",
    "api_key = \"412c37eac090528b3f24fe5843badacb6f3e907f\"  # Remplacez par votre cl√© API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cae81e",
   "metadata": {},
   "source": [
    "### üìÇ √âtape 2 : Charger les donn√©es des stations de v√©los\n",
    "Nous chargeons ici les fichiers contenant les m√©tadonn√©es des stations de v√©los, comme leur position, leur capacit√© et leur ville associ√©e. Ces informations sont utilis√©es plus tard pour enrichir les donn√©es d'utilisation des v√©los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"..\\data_files\"\n",
    "data_directory = r\"..\\data_files\\data\"\n",
    "correct_header_data_bikes = [\"city\", \"id\", \"request_date\", \"datetime\", \"bikes\"]\n",
    "\n",
    "# Open and read data files from the specified directory\n",
    "station_data = DataTools.open_files_in_directory(path, \"bike_station\", \"\\t\")\n",
    "bike_station = Data()\n",
    "# Load the opened data into a Data object\n",
    "bike_station.get_data(station_data)\n",
    "# Filter the data by city or station name\n",
    "bike_station.filter_dataframes(\"city\", [\"amiens\", \"marseille\"])\n",
    "del station_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f5969",
   "metadata": {},
   "source": [
    "### üß™ √âtape 3 : Charger les donn√©es de pollution et de m√©t√©o\n",
    "On importe les donn√©es mesur√©es par les stations de pollution et m√©t√©o. Ces donn√©es sont cruciales pour les corr√©lations futures avec l‚Äôutilisation des v√©los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6179d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read data files from the specified directory\n",
    "station_data = DataTools.open_files_in_directory(path, \"pollution_station\", \",\")\n",
    "pollution_station = Data()\n",
    "# Load the opened data into a Data object\n",
    "pollution_station.get_data(station_data)\n",
    "# Filter the data by city or station name\n",
    "pollution_station.filter_dataframes(\"city\", [\"amiens\", \"marseille\"])\n",
    "del station_data\n",
    "\n",
    "# Open and read data files from the specified directory\n",
    "weather_data = DataTools.open_files_in_directory(data_directory, f\"weather_{formatted_time}\", \",\")\n",
    "data_weather = Data()\n",
    "# Load the opened data into a Data object\n",
    "data_weather.get_data(weather_data)\n",
    "# Filter the data by city or station name\n",
    "data_weather.filter_dataframes(\"name\", [\"Amiens\", \"Marseille\"])\n",
    "del weather_data\n",
    "\n",
    "# Open and read data files from the specified directory\n",
    "pollution_data = DataTools.open_files_in_directory(data_directory, f\"pollution_{formatted_time}\", \",\")\n",
    "data_pollution = Data()\n",
    "# Load the opened data into a Data object\n",
    "data_pollution.get_data(pollution_data)\n",
    "# Filter the data by city or station name\n",
    "data_pollution.filter_dataframes(\"name\", [\"Amiens\"])\n",
    "del pollution_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71174b",
   "metadata": {},
   "source": [
    "### üö≤ √âtape 4 : Charger et fusionner les donn√©es des v√©los\n",
    "Les donn√©es d‚Äôutilisation des v√©los (nombre de v√©los disponibles, etc.) sont charg√©es. Ensuite, on les enrichit en fusionnant avec les m√©tadonn√©es des stations pour ajouter la g√©olocalisation, la capacit√© et l‚Äôidentifiant de pollution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read data files from the specified directory\n",
    "bike_data = DataTools.open_files_in_directory(data_directory, f\"bike_{formatted_time}\", \"\\t\")\n",
    "# Standardize column headers in the bike data\n",
    "bike_data = DataTools.rename_header(bike_data, correct_header_data_bikes, keep_old_header=True)\n",
    "\n",
    "data_bike = Data()\n",
    "# Load the opened data into a Data object\n",
    "data_bike.get_data(bike_data)\n",
    "# Filter the data by city or station name\n",
    "data_bike.filter_dataframes(\"city\", [\"amiens\", \"marseille\"])\n",
    "del bike_data\n",
    "\n",
    "# Merge additional information (e.g., location, capacity) into bike or pollution data\n",
    "data_bike.data = DataTools.merge_dataframes(\n",
    "    data_bike.data, bike_station.data, \"id\", \"id\", [\"bike_stands\", \"latitude\", \"longitude\", \"id_pollution\"]\n",
    ")\n",
    "\n",
    "# Merge additional information (e.g., location, capacity) into bike or pollution data\n",
    "data_pollution.data = DataTools.merge_dataframes(\n",
    "    data_pollution.data, pollution_station.data, \"id\", \"id\", [\"latitude\", \"longitude\"]\n",
    ")\n",
    "\n",
    "del pollution_station\n",
    "del bike_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84627907",
   "metadata": {},
   "source": [
    "### üìä √âtape 5 : Calculer la capacit√© et les statistiques d'utilisation des v√©los\n",
    "Nous analysons l‚Äôutilisation des v√©los par ville : combien de v√©los sont disponibles, combien sont utilis√©s quotidiennement ou √† diff√©rentes p√©riodes de la journ√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of bike slots available in the specified city\n",
    "bike_count_amiens = DataTools.calul_capacity(data_bike.data, \"amiens\")\n",
    "# Calculate the total number of bike slots available in the specified city\n",
    "bike_count_marseille = DataTools.calul_capacity(data_bike.data, \"marseille\")\n",
    "\n",
    "print(f\"Number of slots in Amiens: {bike_count_amiens}\")\n",
    "print(f\"Number of slots in Marseille: {bike_count_marseille}\")\n",
    "\n",
    "# Compute bike usage statistics such as daily and hourly usage\n",
    "dailyuse_amiens, period_use_amiens, useperhour_amiens = DataTools.calculate_use(\n",
    "    data_bike.data[data_bike.data[\"city\"] == \"amiens\"]\n",
    ")\n",
    "# Compute bike usage statistics such as daily and hourly usage\n",
    "dailyuse_marseille, period_use_marseille, useperhour_marseille = DataTools.calculate_use(\n",
    "    data_bike.data[data_bike.data[\"city\"] == \"marseille\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af20ef",
   "metadata": {},
   "source": [
    "### üîç √âtape 6 : R√©aliser une analyse de corr√©lation\n",
    "Nous examinons les liens entre l‚Äôutilisation des v√©los et diff√©rents param√®tres m√©t√©orologiques (temp√©rature, humidit√©...) ou environnementaux (polluants comme NO2, O3...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform correlation analysis between bike usage and environmental factors\n",
    "DataTools.corr_analysis(\n",
    "    [dailyuse_amiens, data_weather.data],\n",
    "    [\"total_bikes_used\", [\"temp\", \"temp_max\", \"temp_min\", \"humidity\", \"speed\", \"clouds\"]],\n",
    ")\n",
    "\n",
    "# Perform correlation analysis between bike usage and environmental factors\n",
    "DataTools.corr_analysis(\n",
    "    [dailyuse_amiens, data_pollution.data],\n",
    "    [\"total_bikes_used\", [\"NO\", \"NO2\", \"NOX as NO2\", \"O3\", \"PM10\", \"PM2.5\"]],\n",
    ")\n",
    "\n",
    "# Perform correlation analysis between bike usage and environmental factors\n",
    "DataTools.corr_analysis(\n",
    "    [dailyuse_marseille, data_weather.data],\n",
    "    [\"total_bikes_used\", [\"temp\", \"temp_max\", \"temp_min\", \"humidity\", \"speed\", \"clouds\"]],\n",
    ")\n",
    "\n",
    "# Perform correlation analysis between bike usage and environmental factors\n",
    "DataTools.corr_analysis(\n",
    "    [dailyuse_marseille, data_pollution.data],\n",
    "    [\"total_bikes_used\", [\"NO\", \"NO2\", \"NOX as NO2\", \"O3\", \"PM10\", \"PM2.5\"]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03f40d",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "### ü§ñ √âtape 7 : Pr√©dire l'utilisation des v√©los\n",
    "Dans cette √©tape, nous utilisons des mod√®les de r√©gression et de classification pour pr√©dire l'utilisation des v√©los en fonction des donn√©es m√©t√©orologiques et environnementales. Nous √©valuons √©galement les performances des mod√®les et effectuons des pr√©dictions sur de nouvelles donn√©es. Les mod√®les peuvent √™tre sauvegard√©s pour une utilisation future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = DataTools.predict_bike_usage(\n",
    "    usage_data=dailyuse_amiens, \n",
    "    weather_data=data_weather.data,\n",
    "    pollution_data=data_pollution.data,\n",
    "    threshold=100 # Optional: value above which usage is considered \"high\"\n",
    ")\n",
    "# After calling predict_bike_usage, add the following code to use the results:\n",
    "# Check if the prediction was successful\n",
    "if results and 'error' not in results:\n",
    "    print(\"\\n===== USING PREDICTION RESULTS =====\")\n",
    "    \n",
    "    # 1. Access the regression model metrics\n",
    "    if 'regression' in results:\n",
    "        reg_metrics = results['regression']\n",
    "        print(f\"Regression model R¬≤ score: {reg_metrics['r2']:.4f}\")\n",
    "        print(f\"RMSE: {reg_metrics['rmse']:.2f} bikes\")\n",
    "        \n",
    "        # Make a prediction with the regression model for a new data point\n",
    "        # Example: predict bike usage for a specific set of features\n",
    "        new_data = pd.DataFrame({\n",
    "            'day_of_week': [0],  # Monday\n",
    "            'month': [6],        # June\n",
    "            'is_weekend': [0],   # Not weekend\n",
    "            'temp': [25],        # 25¬∞C\n",
    "            'humidity': [50]     # 50% humidity\n",
    "            # Add other features as needed\n",
    "        })\n",
    "        \n",
    "        # Make sure to use only the features that the model was trained on\n",
    "        missing_cols = set(reg_metrics['features']) - set(new_data.columns)\n",
    "        for col in missing_cols:\n",
    "            new_data[col] = 0  # Fill missing columns with default values\n",
    "            \n",
    "        new_data = new_data[reg_metrics['features']]  # Reorder columns\n",
    "        \n",
    "        # Scale the data using the same scaler used for training\n",
    "        scaled_data = reg_metrics['scaler'].transform(new_data)\n",
    "        \n",
    "        # Make prediction\n",
    "        predicted_usage = reg_metrics['model'].predict(scaled_data)[0]\n",
    "        print(f\"Predicted bike usage for a 25¬∞C Monday in June: {predicted_usage:.0f} bikes\")\n",
    "    \n",
    "    # 2. Use the classification model for high/low usage prediction\n",
    "    if 'classification' in results:\n",
    "        print(\"\\nBinary classification performance:\")\n",
    "        print(f\"Accuracy: {results['classification']['accuracy']:.2f}\")\n",
    "        \n",
    "        # You could predict if usage will be high or low for new data\n",
    "        if reg_metrics and 'model' in results['classification']:\n",
    "            high_usage = results['classification']['model'].predict(scaled_data)[0]\n",
    "            print(f\"High usage day? {'Yes' if high_usage else 'No'}\")\n",
    "    \n",
    "    # 3. Use the category classification model\n",
    "    if 'category_classification' in results:\n",
    "        print(\"\\nCategory classification performance:\")\n",
    "        print(f\"Accuracy: {results['category_classification']['accuracy']:.2f}\")\n",
    "        \n",
    "        # Predict usage category (low/medium/high) for new data\n",
    "        if reg_metrics and 'model' in results['category_classification']:\n",
    "            category = results['category_classification']['model'].predict(scaled_data)[0]\n",
    "            print(f\"Usage category: {category}\")\n",
    "    \n",
    "    # 4. You could save the models for future use\n",
    "# Importing necessary modules\n",
    "    import pickle\n",
    "    \n",
    "    # Save regression model\n",
    "    if 'regression' in results:\n",
    "        with open('bike_usage_regression_model.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': results['regression']['model'],\n",
    "                'scaler': results['regression']['scaler'],\n",
    "                'features': results['regression']['features']\n",
    "            }, f)\n",
    "        print(\"\\nRegression model saved to 'bike_usage_regression_model.pkl'\")\n",
    "        \n",
    "    print(\"\\n===== END OF RESULTS USAGE =====\")\n",
    "else:\n",
    "    print(\"Prediction failed or returned no results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb00c4",
   "metadata": {},
   "source": [
    "### üöÄ Etape 8 : Side Project\n",
    "\n",
    "Nous avons avanc√© sur des parties du code qui peuvent √™tre int√©gr√©es avec le reste pour une utilisation plus r√©elle et moins acad√©mique. Ces am√©liorations visent √† rendre le projet plus pratique et applicable dans des contextes concrets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c607086",
   "metadata": {},
   "source": [
    "Ici, nous appelons des fonctions qui nous permettent de tracer sur une carte toutes les stations de v√©lo avec le taux de remplissage √† un temps donn√©. Les marqueurs affichent les stations avec une vue maps ou satellite. Si l'on passe la souris sur un marqueur, un popup s'ouvre indiquant le nombre de places, le nombre de v√©los et le taux de remplissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79072209",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = data_bike.data.drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "# Create map with bike stations\n",
    "DataTools.create_bike_station_map(\n",
    "    station_data_list=[station_data],\n",
    "    bike_data_list=[data_bike.data],\n",
    "    specified_time=specified_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91974622",
   "metadata": {},
   "source": [
    "Ici, nous faisons un appel √† l'API publique de JCDecaux qui g√®re les v√©los d'Amiens. Les donn√©es sont r√©cup√©r√©es puis stock√©es dans un fichier Excel. Ces donn√©es sont mises √† jour toutes les minutes, donc une boucle while nous permet de g√©n√©rer un fichier par minute.  \n",
    "Ces donn√©es peuvent ensuite directement √™tre utilis√©es par les autres fonctions, par exemple pour pr√©dire l'utilisation des v√©los durant l'heure suivante (chose non impl√©ment√©e ici)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Liste des contrats disponibles :\")\n",
    "DataTools.get_api_data(contracts_url, api_key)\n",
    "\n",
    "# R√©cup√©rer les stations pour Marseille et Amiens\n",
    "marseille_stations = DataTools.get_station_names(api_key, \"Marseille\")\n",
    "amiens_stations = DataTools.get_station_names(api_key, \"Amiens\")\n",
    "toulouse_stations = DataTools.get_station_names(api_key, \"Toulouse\")\n",
    "\n",
    "while(True):\n",
    "    start_time = time.time()\n",
    "    # Sauvegarder les donn√©es dans un fichier CSV pour Marseille et Amiens\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime(\"%Y_%m_%d_%H_%M\")  # Format : annee_mois_jour_heure_minute\n",
    "    \n",
    "    if marseille_stations:\n",
    "        DataTools.save_station_data_to_xlsx(marseille_stations, \"Marseille\", api_key, date_str)\n",
    "\n",
    "    # if toulouse_stations:\n",
    "    #    DataTools.save_station_data_to_xlsx(toulouse_stations, \"Toulouse\", api_key, date_str)\n",
    "\n",
    "    if amiens_stations:\n",
    "        DataTools.save_station_data_to_xlsx(amiens_stations, \"Amiens\", api_key, date_str)\n",
    "        \n",
    "    time.sleep(max(0, 60 - (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba700c",
   "metadata": {},
   "source": [
    "### ‚úÖ √âtape finale : Nettoyage\n",
    "Pour conclure, nous lib√©rons explicitement la m√©moire utilis√©e pour assurer de bonnes performances et √©viter les fuites m√©moire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c10840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly free memory\n",
    "gc.collect()\n",
    "print(\"Program completed successfully. Memory freed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
